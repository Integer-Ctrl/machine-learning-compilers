Submission 2025-05-08
=====================

SIMD Lanes
----------

This section considers matrix-matrix multiplications, that require instructions where only a subset of SIMD lanes are active.

1. Implement a kernel for M=14, N=6 and K=64 and wrap it in the matmul_14_6_64 function
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

File: ``neon_4_1.s``

For this kernel ``matmul_14_6_64`` we adapt the already implemented kernel ``matmul_16_6_64``. The only change is that we now use 3 ``fmla`` instructions that operate on 4 scalars, and one ``fmla`` instruction that only uses 2 scalars: :math:`4 \cdot 3 + 1 \cdot 2 = 14`.

We load the full 16 floats and ignore the last 2:

.. code-block:: asm
    :linenos:

    ...
    // Load first column from the 14x6 matrix c - load full 16 entries - ignore last 2
    ld1 {v25.4s, v26.4s, v27.4s, v28.4s}, [x2], x5
    // Load second column from the 14x6 matrix c
    ld1 {v17.4s, v18.4s, v19.4s, v20.4s}, [x2], x5
    // Load third column from the 14x6 matrix c
    ld1 {v21.4s, v22.4s, v23.4s, v24.4s}, [x2], x5
    // Load fourth column from the 14x6 matrix c
    ld1 {v5.4s, v6.4s, v7.4s, v8.4s}, [x2], x5
    // Load fifth column from the 14x6 matrix c
    ld1 {v9.4s, v10.4s, v11.4s, v12.4s}, [x2], x5
    // Load sixth column from the 14x6 matrix c
    ld1 {v13.4s, v14.4s, v15.4s, v16.4s}, [x2], x5
    ...

Next the loop over K:

.. code-block:: asm
    :linenos:

    ...
        mov x9, #64 // x9 iterator for K loop
    matmul_loop_over_K:
        sub x9, x9, #1

        // Load first column data from the 14x1 matrix a (again 16 but we'll only using two from v3)
        ld1 {v0.4s, v1.4s, v2.4s, v3.4s}, [x0], x3

        // run the known matmul_16_6_1_unrolled kernel with modification to matmult_14_6_1
        // Load first element from the 1x6 matrix b
        ldr s4, [x1]
        add x1, x1, x4

        // Calculate first column of c
        fmla v25.4s, v0.4s, v4.s[0] // 4 floats
        fmla v26.4s, v1.4s, v4.s[0] // 4 floats
        fmla v27.4s, v2.4s, v4.s[0] // 4 floats
        fmla v28.2s, v3.2s, v4.s[0] // 2 floats

        // Load second element from the 1x6 matrix b
        ldr s4, [x1]
        add x1, x1, x4

        // Calculate second column of c
        fmla v17.4s, v0.4s, v4.s[0]
        fmla v18.4s, v1.4s, v4.s[0]
        fmla v19.4s, v2.4s, v4.s[0]
        fmla v20.2s, v3.2s, v4.s[0]
    ...

We store the full 16 computed floats back to memory but only add an offset of 14 floats because the last two floats aren't used. The last 14 values are exactly stored (8+4+2).

.. code-block:: asm
    :linenos:
    ...
    // Store first column back to memory
    st1 {v25.4s, v26.4s, v27.4s, v28.4s}, [x2], x5 // offset of 14 floats
    // Store second column back to memory
    st1 {v17.4s, v18.4s, v19.4s, v20.4s}, [x2], x5 // offset of 14 floats
    // Store third column back to memory
    st1 {v21.4s, v22.4s, v23.4s, v24.4s}, [x2], x5 // offset of 14 floats
    // Store fourth column back to memory
    st1 {v5.4s, v6.4s, v7.4s, v8.4s}, [x2], x5 // offset of 14 floats
    // Store fifth column back to memory
    st1 {v9.4s, v10.4s, v11.4s, v12.4s}, [x2], x5 // offset of 14 floats
    // Store sixth column back to memory (exactly last 14 elements)
    stp q13, q14, [x2] // 8 floats
    str q15, [x2, #32] // 4 floats
    str d16, [x2, #48] // 2 floats
    ...

2. Implement a kernel for M=15, N=6 and K=64 and wrap it in the matmul_15_6_64 function
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

File: ``neon_4_2.s``

For this kernel ``matmul_15_6_64`` we adapt the already implemented kernel ``matmul_16_6_64``. The only change is that we ignore the last computed float value from the 4 ``fmla`` instructions when saving back to memory.

We load the full 16 floats and ignore the last one:

.. code-block:: asm
    :linenos:

    ...
    // Load first column from the 15x6 matrix c - load full 16 entries - ignore last
    ld1 {v25.4s, v26.4s, v27.4s, v28.4s}, [x2], x5
    // Load second column from the 15x6 matrix c
    ld1 {v17.4s, v18.4s, v19.4s, v20.4s}, [x2], x5
    // Load third column from the 15x6 matrix c
    ld1 {v21.4s, v22.4s, v23.4s, v24.4s}, [x2], x5
    // Load fourth column from the 15x6 matrix c
    ld1 {v5.4s, v6.4s, v7.4s, v8.4s}, [x2], x5
    // Load fifth column from the 15x6 matrix c
    ld1 {v9.4s, v10.4s, v11.4s, v12.4s}, [x2], x5
    // Load sixth column from the 15x6 matrix c
    ld1 {v13.4s, v14.4s, v15.4s, v16.4s}, [x2], x5
    ...

Next the loop over K:

.. code-block:: asm
    :linenos:

    ...
        mov x9, #64 // x9 iterator for K loop
    matmul_loop_over_K:
        sub x9, x9, #1

        // Load first column data from the 15x1 matrix a
        ld1 {v0.4s, v1.4s, v2.4s, v3.4s}, [x0], x3
        // ldp q0, q1, [x0] // 4 + 4 values
        // ldr q2, [x0, #32] // 4 values
        // ldr d3, [x0, #48] // 2 values

        // run the known matmul_16_6_1_unrolled kernel with modification to matmult_15_6_1
        // Load first element from the 1x6 matrix b
        ldr s4, [x1]
        add x1, x1, x4

        // Calculate first column of c
        fmla v25.4s, v0.4s, v4.s[0]
        fmla v26.4s, v1.4s, v4.s[0]
        fmla v27.4s, v2.4s, v4.s[0]
        fmla v28.4s, v3.4s, v4.s[0]

        // Load second element from the 1x6 matrix b
        ldr s4, [x1]
        add x1, x1, x4

        // Calculate second column of c
        fmla v17.4s, v0.4s, v4.s[0]
        fmla v18.4s, v1.4s, v4.s[0]
        fmla v19.4s, v2.4s, v4.s[0]
        fmla v20.4s, v3.4s, v4.s[0]
    ...

We store the full 16 computed floats back to memory but only add an offset of 15 floats because the last float isn't used. The last 15 values are exactly stored (8+4+2+1).

.. code-block:: asm
    :linenos:

    ...
    // Store first column back to memory
    st1 {v25.4s, v26.4s, v27.4s, v28.4s}, [x2], x5 // offset of 15 floats
    // Store second column back to memory
    st1 {v17.4s, v18.4s, v19.4s, v20.4s}, [x2], x5 // offset of 15 floats
    // Store third column back to memory
    st1 {v21.4s, v22.4s, v23.4s, v24.4s}, [x2], x5 // offset of 15 floats
    // Store fourth column back to memory
    st1 {v5.4s, v6.4s, v7.4s, v8.4s}, [x2], x5 // offset of 15 floats
    // Store fifth column back to memory
    st1 {v9.4s, v10.4s, v11.4s, v12.4s}, [x2], x5 // offset of 15 floats
    // Store sixth column back to memory (exactly last 15 elements)
    stp q13, q14, [x2] // 8 floats
    str q15, [x2, #32] // 4 floats
    str d16, [x2, #48] // 2 floats
    mov w9, v16.s[2]
    str w9, [x2, #56] // 1 floats
    ...

3. Test and optimize the kernels. Report your performance in GFLOPS
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Optimized benchmark results:

.. code-block:: asm
    :emphasize-lines: 4, 8

    --------------------------------------------------------------------------------------------------------------------------------------------
    Benchmark                                                                                       Time             CPU   Iterations      FLOPS
    --------------------------------------------------------------------------------------------------------------------------------------------
    GemmMxNxKFixture<14, 6, 64>/BM_matmul_14_6_64/min_warmup_time:1.000_mean                     94.8 ns         94.5 ns           10 113.789G/s
    GemmMxNxKFixture<14, 6, 64>/BM_matmul_14_6_64/min_warmup_time:1.000_median                   94.8 ns         94.5 ns           10 113.775G/s
    GemmMxNxKFixture<14, 6, 64>/BM_matmul_14_6_64/min_warmup_time:1.000_stddev                  0.671 ns        0.659 ns           10 790.609M/s
    GemmMxNxKFixture<14, 6, 64>/BM_matmul_14_6_64/min_warmup_time:1.000_cv                       0.71 %          0.70 %            10      0.69%
    GemmMxNxKFixture<15, 6, 64>/BM_matmul_15_6_64/min_warmup_time:1.000_mean                     95.5 ns         95.1 ns           10 121.074G/s
    GemmMxNxKFixture<15, 6, 64>/BM_matmul_15_6_64/min_warmup_time:1.000_median                   95.4 ns         95.1 ns           10  121.09G/s
    GemmMxNxKFixture<15, 6, 64>/BM_matmul_15_6_64/min_warmup_time:1.000_stddev                  0.295 ns        0.293 ns           10 373.529M/s
    GemmMxNxKFixture<15, 6, 64>/BM_matmul_15_6_64/min_warmup_time:1.000_cv                       0.31 %          0.31 %            10      0.31%

- **matmul_14_6_64** kernel: :math:`113.8` GFLOPS
- **matmul_15_6_64** kernel: :math:`121.1` GFLOPS

Accumulator Block Shapes
------------------------

This section considers a matrix-matrix multiplication where a high-performance implementation may require accumulator blocks with different shapes.

1. Implement a kernel for M=15, N=6 and K=64 and wrap it in the matmul_64_64_64 function
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

File: ``neon_5_1.s``

matmul_64_48_64

For this kernel ``matmul_64_64_64`` we adapt the already implemented kernel ``matmul_64_48_64``. The only changes is that we removed two ``fmla`` blocks from the inner loop:

.. code-block:: asm
    :linenos:
    
    ...
        mov x15, #64 // x15 iterator for K loop
    matmul_loop_over_K:
        sub x15, x15, #1

        // Load first column data from the 16x1 matrix a
        ld1 {v0.4s, v1.4s, v2.4s, v3.4s}, [x0], x3

        // run the matmul_16_4_1_unrolled kernel
        // Load first element from the 1x4 matrix b
        ldr s4, [x1]
        add x1, x1, x4

        // Calculate first column of c
        fmla v25.4s, v0.4s, v4.s[0]
        fmla v26.4s, v1.4s, v4.s[0]
        fmla v27.4s, v2.4s, v4.s[0]
        fmla v28.4s, v3.4s, v4.s[0]


        // Load second element from the 1x4 matrix b
        ldr s4, [x1]
        add x1, x1, x4

        // Calculate second column of c
        fmla v17.4s, v0.4s, v4.s[0]
        fmla v18.4s, v1.4s, v4.s[0]
        fmla v19.4s, v2.4s, v4.s[0]
        fmla v20.4s, v3.4s, v4.s[0]

        
        // Load third element from the 1x4 matrix b
        ldr s4, [x1]
        add x1, x1, x4

        // Calculated third column of c
        fmla v21.4s, v0.4s, v4.s[0]
        fmla v22.4s, v1.4s, v4.s[0]
        fmla v23.4s, v2.4s, v4.s[0]
        fmla v24.4s, v3.4s, v4.s[0]


        // Load fourth element from the 1x4 matrix b
        ldr s4, [x1]
        add x1, x1, x4

        // Calculate fourth column of c
        fmla v5.4s, v0.4s, v4.s[0]
        fmla v6.4s, v1.4s, v4.s[0]
        fmla v7.4s, v2.4s, v4.s[0]
        fmla v8.4s, v3.4s, v4.s[0]


        // offset x6 to the next element in the column
        add x6, x6, #4 // #4 = sizeof(float)

        // Restore x1 to be incremented again
        mov x1, x6

        // Loop back to K
        cbnz x15, matmul_loop_over_K
    ...

Then changed the number of loops over M to four :math:`4 \cdot 16 = 64`:

.. code-block:: asm
    :linenos:
    
    ...
        mov x16, #4 // x16 iterator for M loop
    matmul_loop_over_M:
        sub x16, x16, #1

        // Load first column from the 16x6 matrix c
        ld1 {v25.4s, v26.4s, v27.4s, v28.4s}, [x2], x5
        // Load second column from the 16x6 matrix c
        ld1 {v17.4s, v18.4s, v19.4s, v20.4s}, [x2], x5
        // Load third column from the 16x6 matrix c
        ld1 {v21.4s, v22.4s, v23.4s, v24.4s}, [x2], x5
        // Load fourth column from the 16x6 matrix c
        ld1 {v5.4s, v6.4s, v7.4s, v8.4s}, [x2], x5

        mov x15, #64 // x15 iterator for K loop
    matmul_loop_over_K:
        sub x15, x15, #1
    ...

And finaly changed the number of loops over N to 16 :math:`16 \cdot 4 = 64`:

.. code-block:: asm
    :linenos:
    
    ...
        mov x17, #16 // x17 iterator for N loop
    matmul_loop_over_N:
        sub x17, x17, #1

        mov x16, #4 // x16 iterator for M loop
    matmul_loop_over_M:
        sub x16, x16, #1
    ...

2. Test and optimize the kernel. Report your performance in GFLOPS
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Optimized benchmark result:

.. code-block:: asm
    :emphasize-lines: 4, 8

    --------------------------------------------------------------------------------------------------------------------------------------------
    Benchmark                                                                                       Time             CPU   Iterations      FLOPS
    --------------------------------------------------------------------------------------------------------------------------------------------
    GemmMxNxKFixture<64, 64, 64>/BM_matmul_64_64_64/min_warmup_time:1.000_mean                   4111 ns         4097 ns           10 127.964G/s
    GemmMxNxKFixture<64, 64, 64>/BM_matmul_64_64_64/min_warmup_time:1.000_median                 4110 ns         4096 ns           10 127.988G/s
    GemmMxNxKFixture<64, 64, 64>/BM_matmul_64_64_64/min_warmup_time:1.000_stddev                 13.7 ns         13.8 ns           10 431.794M/s
    GemmMxNxKFixture<64, 64, 64>/BM_matmul_64_64_64/min_warmup_time:1.000_cv                     0.33 %          0.34 %            10      0.34%

- **matmul_14_64_64** kernel: :math:`128.0` GFLOPS

Microkernel
-----------

1. Implement generate function, support only the setting of an FP32 microkernel for C+=AB for M=16, N=6, K=1 and test for errors
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

1. Add support for k parameter by generating a K loop around the microkernel
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   
1. Test the kernel generation. Report performance in GFLOPS
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^